\documentclass{uwstat572}

%%\setlength{\oddsidemargin}{0.25in}
%%\setlength{\textwidth}{6in}
%%\setlength{\topmargin}{0.5in}
%%\setlength{\textheight}{9in}

\renewcommand{\baselinestretch}{1.5} 

\usepackage{amsmath,amsfonts,amsthm,amssymb,amscd}
%\usepackage[margin=1.15in]{geometry}
\usepackage{mathrsfs}
\usepackage{bbm}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{cancel}
\usepackage{listings}
\usepackage{booktabs}

% Begin/End
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\benn}{\begin{equation*}}
\newcommand{\eenn}{\end{equation*}}

% Theorem-like Environments
\newtheorem{thm}{Theorem}[section]
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{exa}[thm]{Example}
\newtheorem{defi}[thm]{Definition}
\newtheorem{exe}[thm]{Exercise}
\newtheorem{rek}[thm]{Remark}
\newtheorem{que}[thm]{Question}
\newtheorem{prob}[thm]{Problem}
\newtheorem{cla}[thm]{Claim}
\newtheorem{esti}[thm]{Estimation}
\newtheorem{assu}[thm]{Assumption}

% Two-Case and Three-Case Definitions
\newcommand{\twocase}[5]{#1 \begin{cases} #2 & \text{#3}\\ #4
&\text{#5} \end{cases}   }
\newcommand{\threecase}[7]{#1 \begin{cases} #2 &
\text{#3}\\ #4 &\text{#5}\\ #6 &\text{#7} \end{cases}   }

% Fractions
\newcommand{\foh}{\frac{1}{2}}
\newcommand{\foq}{\frac{1}{4}}
\newcommand{\fon}{\frac{1}{n}}

% Blackboard Letters
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}
%\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathbb{F}}
%\newcommand{\W}{\mathbb{W}}
\newcommand{\sph}{\mathbb{S}}

% Greek Letters
\newcommand{\ga}{\alpha}
\newcommand{\gb}{\beta}
\newcommand{\gep}{\epsilon}
\newcommand{\vgep}{\varepsilon}
\newcommand{\gth}{\theta}
\newcommand{\vgth}{\vartheta}
\newcommand{\gk}{\kappa}
\newcommand{\gl}{\lambda}
\newcommand{\gs}{\sigma}

\newcommand{\bgb}{\bs{\gb}}

% Matrix
\newcommand{\mattwo}[4]
{\left(\begin{array}{cc}
                        #1  & #2   \\
                        #3 &  #4
                          \end{array}\right) }
\newcommand{\matthree}[9]
{\left(\begin{array}{ccc}
                        #1  & #2 & #3  \\
                        #4 &  #5 & #6 \\
                        #7 & #8 & #9 \\
                          \end{array}\right) }

% Linear Algebra
\newcommand{\col}{\mathrm{col} \,}
\newcommand{\im}{\mathrm{im} \,}
\newcommand{\rank}{\mathrm{rank} \,}
\newcommand{\trace}{\mathrm{tr} \,}
\newcommand{\laspan}{\mathrm{span} \,}
\newcommand{\iprod}[2]{\big\langle #1, \, #2 \big\rangle}
\newcommand{\tr}{\mathrm{T}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\Id}{\bs{I}}

% Calculus
\newcommand{\dd}[2]{\frac{d#1}{d#2}}
\newcommand{\pdd}[2]{\frac{\partial #1}{\partial #2}}

% Operators
\newcommand{\Ad}{\mathrm{Ad}}
\newcommand{\Lie}[1]{\mathrm{Lie}(#1)}
\newcommand{\sign}{\mathrm{sign} \,}
\newcommand{\supp}{\mathrm{supp} \,}

% Geometry
\newcommand{\del}[2]{\nabla_{#1}#2}
\newcommand{\multi}[1]{\underset{\raisebox{2pt}[0pt]{$\rightharpoondown$}}{#1}}
\newcommand{\multis}[1]{\underset{\raisebox{2pt}[2pt]{$\scriptscriptstyle\rightharpoondown$}}{#1}}

% Probability/Statistics
\newcommand{\Prb}{\mathrm{P}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Ehat}{\hat{\E}}
\newcommand{\Ex}{\mathbb{E}}
\newcommand{\bias}{\mathrm{bias}}
\newcommand{\var}{\mathrm{var}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\cid}{\overset{d}{\to}}
\newcommand{\X}{\bs{X}}
\newcommand{\Y}{\bs{Y}}
\newcommand{\SE}{\mathrm{SE}}

% Distributions
\newcommand{\Bern}{\mathrm{Bernoulli}}
\newcommand{\Poisson}{\mathrm{Poisson}}
\newcommand{\Normal}{\mathrm{N}}
\newcommand{\DGamma}{\mathrm{Gamma}}
\newcommand{\Unif}{\mathrm{Uniform}}

% Math operators
\DeclareMathOperator*{\argmin}{arg\,min}

% Formatting
\newcommand{\bs}[1]{\boldsymbol{#1}}

% Graphics
\newcommand{\grapher}[2]{\centerline{\includegraphics[width=#2]{#1}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Code Environments

\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\usepackage[normalem]{ulem}
\usepackage{algpseudocode}
\usepackage{graphicx} 
\usepackage{fancyvrb} 
\lstnewenvironment{Rcode}{\lstset{frame=tb,% setup listings 
        language=R,% set programming language 
        basicstyle=\small\ttfamily,% basic font style 
        keywordstyle=\bfseries\color{blue},% keyword style 
        commentstyle=\ttfamily\itshape\color{dkgreen},% comment style 
          stringstyle=\color{mauve},
        numbers=left,% display line numbers on the left side 
        numberstyle=\scriptsize,% use small line numbers 
        numbersep=10pt,% space between line numbers and code 
        tabsize=3,% sizes of tabs 
        showstringspaces=false,% do not replace spaces in strings by a certain character 
        captionpos=b,% positioning of the caption below 
        breaklines=true,% automatic line breaking 
        escapeinside={(*}{*)},% escaping to LaTeX 
        fancyvrb=true,% verbatim code is typset by listings 
        extendedchars=false,% prohibit extended chars (chars of codes 128--255) 
        literate={"}{{\texttt{"}}}1{<-}{{$\leftarrow$}}1{<<-}{{$\twoheadleftarrow$}}1 
        {~}{{$\sim$}}1{<=}{{$\le$}}1{>=}{{$\ge$}}1{!=}{{$\neq$}}1{^}{{$^\wedge$}}1,% item to replace, text, length of chars 
        alsoletter={.<-},% becomes a letter 
        alsoother={$},% becomes other 
        otherkeywords={!=, ~, $, *, \&, \%/\%, \%*\%, \%\%, <-, <<-, /},% other keywords 
        deletekeywords={c}% remove keywords }}{}
        }
        }{}

\numberwithin{equation}{section}
\begin{document}
\nocite{*}

\begin{center}
  {\LARGE Approximately Sparse Econometric Models}\\\ \\
  {David Gold \\ 
    Department of Statistics, University of Washington Seattle, WA, 98195, USA
  }
\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MACROS

\newcommand{\tp}{\mathrm{T}}

\newcommand{\z}{\bs{z}}
\newcommand{\w}{\bs{w}}
\newcommand{\inst}{\bs{u}}
\newcommand{\instc}{u}

\newcommand{\pz}{{p_{\z}}}
\newcommand{\pw}{{p_{\w}}}
\newcommand{\pinst}{{p_{\inst}}}

\newcommand{\reg}{\gth}
\newcommand{\eft}{\gth}
\newcommand{\efthat}{\hat{\eft}}
\newcommand{\nuis}{\bs{\gamma}}
\newcommand{\sdh}{\gs_h}
\newcommand{\sdv}{\gs_v}
\newcommand{\covhv}{\gs_{hv}}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% INTRODUCTION

\section{Introduction}


An important objective of econometrics research is to conduct inference for the effect of a treatment variables on some economic response variable. In general, the value that the treatment takes for a given observation is not prescribed by the analyst. The relationship between the treatment and response may therefore be subject to \emph{confounding}. That is, the distribution of the response given a \emph{hypothetical intervention} in which treatment levels are prescribed by the analyst may differ from the distribution of the response given a natural distribution of treatments. Insofar as the effect of interest is specified in terms of the former distribution, the aforementioned discrepancy can induce bias in estimators derived from the latter distribution. In turn, statistical methods that address bias due to confounding are particularly relevant to econometrics studies. Two such methods are (i) the \emph{method of instrumental variables (IV)} and (ii) inference methods for \emph{partially linear (PL) models}.

%At the level of generality considered in \cite{BCH11}, both methods assume sempiarametric models for the data-generating process of interest; each assumes that certain effects, including that of the treatment, are linear, but does not restrict the functional form of other relationships between covariates. In both models, inference for the linear treatment effect depends on estimation of the nonparametric parts of the model. The present essay studies the use $\ell_1$-regularized methods to estimate the nonparametric components of these models under additional sparsity-based assumptions. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CONFOUNDING

\subsection{Confounding}\label{sec:conf}
\newcommand{\efc}{\bs{\gamma}}
\newcommand{\W}{\bs{W}}
\newcommand{\Z}{\bs{Z}}
\newcommand{\erry}{\gep}
\newcommand{\pd}{\partial}

In this section, we briefly illustrate how confounding bias arises from the confluence of two factors: (i) the specification of a parameter of interest in terms of a hypothetical intervention in which treatment is assigned at the discretion of the analyst and (ii) the difference between such an intervention and actual circumstances. Let $X, Y$ denote treatment and response variables, respectively, and suppose that the true data-generating mechanism of interest is
%\be\label{eq:conf1}
%	\E[Y\,|\,x, \w] \ = \ \gb x + \efc^\tp\w \,, % + \gep\,, 
%\ee
%where $\W\in\R^\pw$ are further covariates that influence $Y$ and $\efc\in\R^\pw$ is a nuisance parameter. One possible quantity of interest is $\eft = \pdd{}{x}\E[Y\,|\,x]$, where the reference population is considered under a hypothetical intervention in which $X$ is set to $x$ at the discretion of the analyst. Under such a hypothetical intervention, in which we assume that $X$ is independent of $\W$, the conditional expectation of $Y$ given $x$ is just $\gb x + \efc^\tp\E[\W]$. Thus the parameter of interest $\eft$ is equal to $\gb$. However, under practical circumstances, $X$ may be associated with $\W$, one finds instead that
%\be
%	\pdd{}{x}\E[Y\,|\,x] \ = \ \pdd{}{x}\big[\gb x + \efc^\tp\E[\W\,|\,x]\big] \ = \ \gb + \efc^\tp\pdd{}{x}\E[\W\,|\,x] \,.
%\ee
%If measurements of the potential confounders $\W$ are available, the analyst may obtain an estimate of $\efc$ by fitting the mean model of \ref{eq:conf1}. However, if such measurements are unavailable, or if the identities of the relevant $\W$ are unknown, then an estimator of $\eft$ obtained by na\"ively fitting the model $\E[Y\,|\,x] = \gb' x$ will incur bias. 
\be\label{eq:conf1}
	\E[Y\,|\,x, \w] \ = \ \gb x + f(\w) \,, % + \gep\,, 
\ee
where $\W\in\R^\pw$ are further covariates that influence $Y$. One possible quantity of interest is $\eft = \pdd{}{x}\E[Y\,|\,x]$, where the reference population is considered under a hypothetical intervention in which $X$ is set to $x$ at the discretion of the analyst. Given that, under such a hypothetical intervention, assignment of the treatment $X$ is independent of $\W$, the parameter of interest $\eft$ is equal to $\gb$. However, under actual circumstances, in which $X$ may be associated with $\W$, measurements of the latter are often unavailable. Estimating $\eft$ by na\"ively fitting the model $\E[Y\,|\,x] = \gb^\dagger x$ incurs a well-known bias that, in general, is not alleviated by asymptotics.

%incurs the bias
%\benn
%	\bias(\hat{\gb^\dagger}) \ = \ P_{X,n}\big(A\times \E[f(\W)\,|\,x]\big) \,, \qquad A \ = \ \frac{x-\mu_{X,n}}{P_{X,n}(x-\mu_{X,n})^2} \,,
%\eenn
%where $P_{X,n}f$ denotes the expectation of $f(X)$ under the empirical distribution $P_{X,n}$ of the observed treatments and $\mu_{X,n}$ is the empirical mean of the observed treatments. In general, asymptotics do not alleviate such bias due to omitted confounders. 
 
The method of instrumental variables addresses the foregoing issues by introducing \emph{instrumental variables} $\Z$ associated with $X$ through the conditional mean function $\E[X\,|\,\z]$ but otherwise unassociated with the response $Y$. In turn, $X$ is allowed to be \emph{endogenous} --- that is, to have nontrivial covariance with the response error. The endogeneity of $X$ reflects the influence of unmeasured confounders. Intuitively, the method of instrumental variables circumvents confounding bias by studying the variation in the response relative to the ``part'' of variation in the treatment that is due only to the effect of the instruments. 

%Whether it is reasonable to assume that proposed instruments for a given treatment effect satisfy the foregoing conditions depends on theoretical or scientific knowledge of the mechanism or population under study. 
%An example is \cite{AK91}, who apply the method of instrumental variables to estimate of the effect of years of schooling on income later in life. Prima facie, it is possible that unmeasured variables that reflect innate academic ability may be associated with both years of schooling and adult income.  

The partially linear model \ref{eq:conf1} is relevant when the analyst is reasonably confident that potential confounders $\W$ are measured but lacks a priori knowledge that would justify restrictions, such as linearity, on the functional relationship between the response $Y$ and $\W$. In such cases the analyst may reasonably assume the $X$ are not endogenous and avail herself of more general semiparametric methods.

Apart from the foregoing motivation, the present essay focuses exclusively on the statistical properties of estimators $\efthat$ derived from the IV and PL models. We refer the reader to \cite[Chapter 5]{P09} and \cite{AK01} for deeper discussions of how instrumental variables models in particular come to bear on causal inference. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% SERIES APPROXIMATION

\subsection{Inference}

Inference for the treatment effect in an IV model proceeds by fitting the observed responses $Y_i$ to estimates $\Ehat[X_i\,|\,\z_i]$. In linear IV models, the simplest implementation of this procedure is \emph{Two-Stage Least Squares (2SLS)} regression of the treatment $X$ on the instruments $\Z$ in the first stage and $Y$ on the estimates $\Ehat[X\,|\,\z]$ in the second stage. Small and large sample properties of the 2SLS have been extensively researched since the 1950s, particularly to the end of alleviating the bias incurred by introducing ``many instruments'' --- that is, letting the number $\pz$ of instruments grow with the sample size $n$. Early work in this vein concerned the \emph{Limited Information Maximum Likelihood (LIML) estimator} [TODO: comment on LIML -- I'm still working through the literature on this]. 

\cite{F77} studies a modification of the LIML estimator that has finite moments and bias of order of $n^{-2}$ [TODO: comment on issues concerning standard error estimates of Fuller estimator]. \cite{B94} proposes similarly modified estimators and obtains corrected standard errors for asymptotic approximations under a particular sequence of model parameters in which $\pz$ may satsify $\lim_{n\to\infty} \pz/n > 0$ --- a condition that entails inconsistency of the 2SLS estimator under reasonable assumptions. More recently, \cite{HHN08} extend the result of \cite{B94} to the case of non-Gaussian errors. \cite{BW11} give an accessible survey of approximations to the finite sample bias of the 2SLS estimator in the case of a single endogenous covariate $X$. 















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Instrumental Variables Models
%
%\subsection{Instrumental Variables Models}
%
%The linear IV model characterizes potential confounding of $x$ and $y$ as a matter of dependence between $x$ and an error term. Formally, we suppose that the observations $(y_i, x_i, \w_i, \z_i)$ for $i\in[n] := \{1,\ldots,n\}$ are independent and identically distributed according to
%\begin{align}
%	y_i \ & = \ x_i\reg + \w^\tp\nuis + h_i \,, \\
%	x_i \ & = \ d(\inst_i) + v_i \,,
%\end{align}
%where
%%where $h_i, v_i$ are mean-zero error terms satisfying $\var(h_i) = \sdh^2$, $\var(v_i) = \sdv^2$, and $\cov(h_i, v_i) = \covhv$. Throughout the present essay we restrict our consideration to the case in which each $h_i, v_i$ are jointly distributed as multivariate Gaussian. 
%\be
%	\begin{pmatrix} h_i \\ v_i \end{pmatrix} \ \sim \ \Normal\left(\bs{0}, \begin{pmatrix} \sdh^2 & \covhv \\ \covhv & \sdv^2 \end{pmatrix}\right) \,.
%\ee
%Here, $\w_i = (w_{i1},\ldots,w_{i\pw})$ is a vector of \emph{control variables} and $\inst_i = (\instc_{i1},\ldots,\instc_{i\pinst})$ is a vector of \emph{instrumental variables} that typically contains $\w_i$ and additional covariates. 
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% 	PARTIALLY LINEAR MODELS
%
%\subsection{Partially Linear Models}
%
%The PL model assumes that potential confounders $\z$ of $x$ and $y$ are observed, but that the functional form by which each variable depends on $\z$ is unspecified. Formally, we suppose that the observations $(y_i, x_i, \z_i)$ are independent and identically distributed according to
%\begin{align}
%	y_i \ & = \ x_i\reg + g(\z_i) + h_i \,, \\
%	x_i \ & = \ d(\z_i) + v_i \,,
%\end{align}
%where
%\be
%	\begin{pmatrix} h_i \\ v_i \end{pmatrix} \ \sim \ \Normal\left(\bs{0}, \begin{pmatrix} \sdh^2 & 0 \\ 0 & \sdv^2 \end{pmatrix}\right) \,.
%\ee
%
%
%
%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% REFERENCES

\bibliographystyle{plainnat}
\bibliography{../references}
%\bibliography{pres_intro}



\end{document}