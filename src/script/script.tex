\documentclass{uwstat572}

%%\setlength{\oddsidemargin}{0.25in}
%%\setlength{\textwidth}{6in}
%%\setlength{\topmargin}{0.5in}
%%\setlength{\textheight}{9in}

\renewcommand{\baselinestretch}{1.5} 

\usepackage{amsmath,amsfonts,amsthm,amssymb,amscd}
%\usepackage[margin=1.15in]{geometry}
\usepackage{mathrsfs}
\usepackage{bbm}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{cancel}
\usepackage{listings}
\usepackage{booktabs}
\usepackage[table]{xcolor}
\usepackage{colortbl}
\usepackage{multirow}

% Begin/End
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\benn}{\begin{equation*}}
\newcommand{\eenn}{\end{equation*}}

% Theorem-like Environments
\newtheorem{thm}{Theorem}[section]
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{cond}[thm]{Condition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{exa}[thm]{Example}
\newtheorem{exe}[thm]{Exercise}

\newtheorem{que}[thm]{Question}
\newtheorem{prob}[thm]{Problem}
\newtheorem{cla}[thm]{Claim}
\newtheorem{esti}[thm]{Estimation}
\newtheorem{assu}[thm]{Assumption}

\theoremstyle{definition}
\newtheorem{defi}[thm]{Definition}

\theoremstyle{remark}
\newtheorem{rek}[thm]{Remark}

% Two-Case and Three-Case Definitions
\newcommand{\twocase}[5]{#1 \begin{cases} #2 & \text{#3}\\ #4
&\text{#5} \end{cases}   }
\newcommand{\threecase}[7]{#1 \begin{cases} #2 &
\text{#3}\\ #4 &\text{#5}\\ #6 &\text{#7} \end{cases}   }

% Fractions
\newcommand{\foh}{\frac{1}{2}}
\newcommand{\foq}{\frac{1}{4}}
\newcommand{\fon}{\frac{1}{n}}

% Blackboard Letters
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}
%\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathbb{F}}
%\newcommand{\W}{\mathbb{W}}
\newcommand{\sph}{\mathbb{S}}

% Greek Letters
\newcommand{\ga}{\alpha}
\newcommand{\gb}{\beta}
\newcommand{\gep}{\epsilon}
\newcommand{\vgep}{\varepsilon}
\newcommand{\gth}{\theta}
\newcommand{\vgth}{\vartheta}
\newcommand{\gk}{\kappa}
\newcommand{\gl}{\lambda}
\newcommand{\gs}{\sigma}

\newcommand{\bgb}{\bs{\gb}}

% Matrix
\newcommand{\mattwo}[4]
{\left(\begin{array}{cc}
                        #1  & #2   \\
                        #3 &  #4
                          \end{array}\right) }
\newcommand{\matthree}[9]
{\left(\begin{array}{ccc}
                        #1  & #2 & #3  \\
                        #4 &  #5 & #6 \\
                        #7 & #8 & #9 \\
                          \end{array}\right) }

% Linear Algebra
\newcommand{\col}{\mathrm{col} \,}
\newcommand{\im}{\mathrm{im} \,}
\newcommand{\rank}{\mathrm{rank} \,}
\newcommand{\trace}{\mathrm{tr} \,}
\newcommand{\laspan}{\mathrm{span} \,}
\newcommand{\iprod}[2]{\big\langle #1, \, #2 \big\rangle}
\newcommand{\tr}{\mathrm{T}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\Id}{\bs{I}}

% Calculus
\newcommand{\dd}[2]{\frac{d#1}{d#2}}
\newcommand{\pdd}[2]{\frac{\partial #1}{\partial #2}}

% Operators
\newcommand{\Ad}{\mathrm{Ad}}
\newcommand{\Lie}[1]{\mathrm{Lie}(#1)}
\newcommand{\sign}{\mathrm{sign} \,}
\newcommand{\supp}{\mathrm{supp} \,}

% Geometry
\newcommand{\del}[2]{\nabla_{#1}#2}
\newcommand{\multi}[1]{\underset{\raisebox{2pt}[0pt]{$\rightharpoondown$}}{#1}}
\newcommand{\multis}[1]{\underset{\raisebox{2pt}[2pt]{$\scriptscriptstyle\rightharpoondown$}}{#1}}

% Probability/Statistics
\newcommand{\Prb}{\mathrm{P}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\En}{\mathbb{E}_n}
\newcommand{\Ehat}{\hat{\E}}
\newcommand{\Ex}{\mathbb{E}}
\newcommand{\Exn}{\Ex_n}
\newcommand{\bias}{\mathrm{bias}}
\newcommand{\var}{\mathrm{var}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\cid}{\overset{d}{\to}}
\newcommand{\X}{\bs{X}}
\newcommand{\Y}{\bs{Y}}
\newcommand{\SE}{\mathrm{SE}}

% Distributions
\newcommand{\Bern}{\mathrm{Bernoulli}}
\newcommand{\Poisson}{\mathrm{Poisson}}
\newcommand{\Normal}{\mathrm{N}}
\newcommand{\DGamma}{\mathrm{Gamma}}
\newcommand{\Unif}{\mathrm{Uniform}}

% Math operators
\DeclareMathOperator*{\argmin}{arg\,min}

% Formatting
\newcommand{\bs}[1]{\boldsymbol{#1}}

% Graphics
\newcommand{\grapher}[2]{\centerline{\includegraphics[width=#2]{#1}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Code Environments

\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\usepackage[normalem]{ulem}
\usepackage{algpseudocode}
\usepackage{graphicx} 
\usepackage{fancyvrb} 
\lstnewenvironment{Rcode}{\lstset{frame=tb,% setup listings 
        language=R,% set programming language 
        basicstyle=\small\ttfamily,% basic font style 
        keywordstyle=\bfseries\color{blue},% keyword style 
        commentstyle=\ttfamily\itshape\color{dkgreen},% comment style 
          stringstyle=\color{mauve},
        numbers=left,% display line numbers on the left side 
        numberstyle=\scriptsize,% use small line numbers 
        numbersep=10pt,% space between line numbers and code 
        tabsize=3,% sizes of tabs 
        showstringspaces=false,% do not replace spaces in strings by a certain character 
        captionpos=b,% positioning of the caption below 
        breaklines=true,% automatic line breaking 
        escapeinside={(*}{*)},% escaping to LaTeX 
        fancyvrb=true,% verbatim code is typset by listings 
        extendedchars=false,% prohibit extended chars (chars of codes 128--255) 
        literate={"}{{\texttt{"}}}1{<-}{{$\leftarrow$}}1{<<-}{{$\twoheadleftarrow$}}1 
        {~}{{$\sim$}}1{<=}{{$\le$}}1{>=}{{$\ge$}}1{!=}{{$\neq$}}1{^}{{$^\wedge$}}1,% item to replace, text, length of chars 
        alsoletter={.<-},% becomes a letter 
        alsoother={$},% becomes other 
        otherkeywords={!=, ~, $, *, \&, \%/\%, \%*\%, \%\%, <-, <<-, /},% other keywords 
        deletekeywords={c}% remove keywords }}{}
        }
        }{}

\numberwithin{equation}{section}
\begin{document}
\nocite{*}
\newcommand{\note}[1]{\textcolor{red}{[NOTE: #1]}}
\newcommand{\todo}[1]{\textcolor{red}{[TO-DO: #1]}}

\begin{center}
  {\LARGE Approximately Sparse Econometric Models}\\\ \\
  {David Gold \\ 
    Department of Statistics, University of Washington Seattle, WA, 98195, USA
  }
\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MACROS

\newcommand{\tp}{\mathrm{T}}

\newcommand{\z}{\bs{z}}
\newcommand{\w}{\bs{w}}
\newcommand{\inst}{\bs{u}}
\newcommand{\instc}{u}

\newcommand{\pz}{{p_{\z}}}
\newcommand{\pw}{{p_{\w}}}
\newcommand{\pinst}{{p_{\inst}}}

\newcommand{\reg}{\gth}
\newcommand{\eft}{\gth}
\newcommand{\efthat}{\hat{\eft}}
\newcommand{\nuis}{\bs{\gamma}}
\newcommand{\sdh}{\gs_h}
\newcommand{\sdv}{\gs_v}
\newcommand{\covhv}{\gs_{hv}}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% INTRODUCTION

\section{Introduction}\label{s:intro}


An important objective of econometrics research is to conduct inference for the effect of a treatment variables on some economic response variable. In general, the value that the treatment takes for a given observation is not prescribed by the analyst. The relationship between the treatment and response may therefore be subject to \emph{confounding}. That is, the distribution of the response given a \emph{hypothetical intervention} in which treatment levels are prescribed by the analyst may differ from the distribution of the response given a natural distribution of treatments. Insofar as the effect of interest is specified in terms of the former distribution, the aforementioned discrepancy can induce bias in estimators derived from the latter distribution. In turn, statistical methods that address bias due to confounding are particularly relevant to econometrics studies. Two such methods are (i) the \emph{method of instrumental variables (IV)} and (ii) inference methods for \emph{partially linear (PL) models}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CONFOUNDING

\subsection{Confounding}\label{ss:conf}
\newcommand{\rg}{\bs{\gamma}}
\newcommand{\W}{\bs{W}}
\newcommand{\Z}{\bs{Z}}
\newcommand{\erry}{\gep}
\newcommand{\pd}{\partial}

In this section, we briefly illustrate how confounding bias arises from the confluence of two factors: (i) the specification of a parameter of interest in terms of a hypothetical intervention in which treatment is assigned at the discretion of the analyst and (ii) the difference between such an intervention and actual circumstances. Let $X, Y$ denote treatment and response variables, respectively, and suppose that the true data-generating mechanism of interest is
\be\label{eq:conf1}
	\E[Y\,|\,x, \w] \ = \ \gb x + f(\w) \,, % + \gep\,, 
\ee
where $\W\in\R^\pw$ are further covariates that influence $Y$. One possible quantity of interest is $\eft = \pdd{}{x}\E[Y\,|\,x]$, where the reference population is considered under a hypothetical intervention in which $X$ is set to $x$ at the discretion of the analyst. Given that, under such a hypothetical intervention, assignment of the treatment $X$ is independent of $\W$, the parameter of interest $\eft$ is equal to $\gb$. However, under actual circumstances, $X$ may be associated with $\W$ and measurements of the latter may be unavailable. In such cases, estimating $\eft$ by na\"ively fitting the model $\E[Y\,|\,x] = \gb^\dagger x$ incurs a well-known bias that, in general, is not alleviated by asymptotics.
 
The method of instrumental variables addresses the foregoing issues by introducing \emph{instrumental variables} $\Z$ associated with $X$ through the conditional mean function $\E[X\,|\,\z]$ but otherwise unassociated with the response $Y$. In turn, $X$ is allowed to be \emph{endogenous} --- that is, to have nontrivial covariance with the response error. (In the sequel, we sometimes abuse terminology and refer to objects associated with $X$ --- for instance, a regression function or the treatment effect --- as ``endogenous.'') The endogeneity of $X$ reflects the influence of unmeasured confounders. Intuitively, the method of instrumental variables circumvents confounding bias by studying the variation in the response relative to the ``part'' of variation in the treatment that is due only to the effect of the instrumental variables. 

%Whether it is reasonable to assume that proposed instruments for a given treatment effect satisfy the foregoing conditions depends on theoretical or scientific knowledge of the mechanism or population under study. 
%An example is \cite{AK91}, who apply the method of instrumental variables to estimate of the effect of years of schooling on income later in life. Prima facie, it is possible that unmeasured variables that reflect innate academic ability may be associated with both years of schooling and adult income.  

The partially linear model \ref{eq:conf1} is relevant when the analyst is reasonably confident that potential confounders $\W$ are measured but lacks a priori knowledge that would justify restrictions, such as linearity, on the functional relationship between the response $Y$ and $\W$. In such cases the analyst may reasonably assume the $X$ are not endogenous and avail herself of more general semiparametric methods.

Apart from the foregoing motivation, the present essay focuses exclusively on the statistical properties of estimators $\efthat$ derived from the IV and PL models. We refer the reader to \cite[Chapter 5]{P09} and \cite{AK01} for deeper discussions of how IV models in particular come to bear on causal inference. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% INFERENCE

\subsection{Inference for IV models}\label{ss:inf}

Inference for the treatment effect in linear IV models is often conducted by fitting the observed responses $Y_i$ to estimates $\Ehat[X_i\,|\,\z_i]$. The simplest implementation of this procedure is \emph{two-stage least squares (2SLS)} regression of the treatment $X$ on the instrumental variables $\Z$ in the first stage and $Y$ on the estimates $\Ehat[X\,|\,\z]$ in the second stage. Early studies of the 2SLS estimator include \cite{T53}, \cite{B57}, \cite{S58}. Such studies tended to consider the 2SLS estimator in a generalized method of moments (GMM) context; the 2SLS estimator can be motivated by the assumption that the instrumental variables and the second-stage errors have zero covariance. 

Early treatments of IV models, such as the aforementioned, occurred in the context of simultaneous equations modeling. \cite{AR50} studied the maximum likelihood estimator of the regression parameters of a single equation in a system of linear equations --- the so-called \emph{limited information maximum likelihood (LIML) estimator}. The authors studied the LIML estimator's asymptotic properties via an approximation that is essentially the 2SLS estimator, though the latter was not their primary object of study. Such an approximation demonstrates $\sqrt{n}$-consistency and asymptotic efficiency of both estimators for linear IV models.

The finite-sample properites of such estimators are noteworthy. \cite{MS72} demonstrate via a study of its exact finite-sample distribution that, for IV models with Gaussian errors and a single endogenous variable other than the response, the LIML estimator does not possess moments. \cite{F77} proposes a class of modified LIML estimators that do possess moments and demonstrates that one such estimator has bias on the order of $n^{-2}$. The standard error estimates suggested by asymptotic approximations for this class of estimators tend to be anti-conservative. \cite{B94} studies similarly modified estimators and obtains corrected standard errors for asymptotic approximations under a particular sequence of model parameters in which $\pz$ may satsify $\lim_{n\to\infty} \pz/n > 0$ --- a condition that entails inconsistency of the 2SLS estimator under reasonable assumptions (see \cite{BW11}).  More recently, \cite{HHN08} extend the results of \cite{B94} to the case of heteroscedastic, non-Gaussian errors.

The presently studied paper, \cite{BCH11}, considers IV models in which regression functions for the endogenous variables may be nonlinear.  \cite{A74}, \cite{A77}, \cite{H82} and \cite{C87} study the asymptotic properties of estimators for nonlinear or nonparametric endogenous regression functions. Such estimators are implicit solutions to optimization problems whose objective functions depend on generic functions --- called \emph{instruments} --- of the instrumental variables. The instruments feature in the asymptotic variance of such estimators, hence the optimality of instruments can be considered in terms of relative asymptotic efficiency. The aforementioned works characterize the optimal instruments as functions of the conditional expectations of the endogenous variables given the instrumental variables. Thus, methods for nonparametric regression come to bear on inference for general IV models. For instance, \cite{N90} studies the use of nearest neighbor and series approximation --- in which an unknown regression function is estimated by an expansion of its argument into a series of basis terms --- to optimal inference in nonlinear IV models. In Section \ref{s:methods}, we will see that \cite{BCH11} extend such use of series estimation by leveraging the model selection capacity of $\ell_1$-regularized estimators of the endogenous regression function. To that end, we introduce notation. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% NOTATION
\subsection{Notation}
\renewcommand{\d}{\bs{d}}
\newcommand{\dhat}{\hat{\d}}
\renewcommand{\dh}{\hat{\d}}
\newcommand{\dhatc}{\hat{d}}
\newcommand{\dhati}{\dhatc_i}
\newcommand{\db}{\d_{\regb}}
\renewcommand{\u}{\bs{u}}
\newcommand{\x}{\bs{x}}
\newcommand{\y}{\bs{y}}
\newcommand{\U}{\bs{U}}
\newcommand{\inn}{\in[n]}
\newcommand{\event}{\mathcal{E}}
\newcommand{\kp}{\otimes}
\newcommand{\kpt}{\kp2}
\newcommand{\B}{\bs{B}}
\newcommand{\ip}[1]{\langle#1\rangle}
\newcommand{\ipp}[2]{#2\langle#1 #2\rangle}

For $p\in\N$, we let $[p] := \{1,\ldots,p\}$. We let bold and non-bold lowercase letters denote vectors and scalars, respectively; we use bold uppercase letters to denote matrices. We typically denote the components of a vector (matrix) by the non-bold (lowercase) counterpart of the letter that denotes the vector (matrix). If $\Z = (z_{ij})_{i,j\in[n]\times[p]}$, we use a superscript to refer to columns $\z^j = (z_{ij})_{i\inn}$ and a subscript to refer to rows $\z_i = (z_{ij})_{j\in[p]}$. We let $\|\cdot\|_p$ denote the usual $\ell_p$ norm over Euclidean spaces.  For $\u \in \R^p$, we let $\supp(\u) := \{j\in[p]: u_j \neq 0\}$ and $\|\u\|_0 = |\supp(\u)|$; we let $\|\u\|_\infty = \max_{j\in[p]} \{u_j\}$. We let $\otimes$ denote the Kronecker product and write $\u^{\kpt} = \u \kp \u$ for $\u\in\R^p$; for conformal matrices $\B$ we write $\|\u\|_{\B} = \ip{\u,\B\u}$. If $x$ is a (random or deterministic) quantity indexed by $i\inn$, we let $\Exn[x_i] = n^{-1}\sum_{i=1}^n x_i$. For $a,b\in\R$, we let $a\vee b = \max\{a,b\}$ and $a\wedge b = \min\{a,b\}$. We write $a_n \lesssim b_n$ if $a_n \leq Cb_n$ for a universal constant $C$; we write $a_n \lesssim_\Prb b_n$ if $a_n = O_\Prb(b_n)$. We say that an event $\event \equiv \event_n$ occurs with high probability if $\lim_{n\to\infty} \Prb \, \event = 1$. 
%we let $\En = \E\circ\Exn$ denote the ``average expectation'' operator


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% METHODS
\newcommand{\px}{p_{\x}}
\newcommand{\pu}{p_{\u}}
\newcommand{\pun}{p_{\u,n}}
\newcommand{\regb}{\bs{\gb}}
\newcommand{\regbh}{\hat{\regb}}
\newcommand{\fb}{f_{\regb}}
\newcommand{\fbn}{f_{\regb, n}}
\newcommand{\fbhat}{\hat{\fb}}
\newcommand{\rem}{\bs{r}}
\newcommand{\remc}{r}
\newcommand{\remi}{\remc_i}
\newcommand{\rfb}{r_{\famb}}
\newcommand{\rc}{c_r}
\newcommand{\rb}{r_{\regb}}
\newcommand{\rbn}{r_{\regb,n}}
\newcommand{\rfs}{r_{\fams}}
\newcommand{\Xs}{\mathcal{X}}
\newcommand{\Zs}{\mathcal{Z}}
\newcommand{\Fs}{\mathcal{F}}
\newcommand{\Bs}{\mathcal{B}}
\newcommand{\famb}{\Fs_\Bs}
\newcommand{\fams}{\Fs_\sb}
\newcommand{\Sb}{S}
\newcommand{\Us}{\mathcal{U}}
\renewcommand{\sb}{s}
\newcommand{\gramhat}{\hat{\bs{\Sigma}}}
\newcommand{\tune}{\lambda}
\newcommand{\esi}{u_i}
\newcommand{\sd}{\gs}
\newcommand{\f}{\bs{f}}
\newcommand{\fhat}{\hat{\f}}
\newcommand{\fhc}{\hat{f}}
\newcommand{\ffb}{\bs{f}_{\regb}}

\section{Methods}\label{s:methods}

In Section \ref{ss:inf}, we discussed how IV and PL models come to bear on econometrics research. Efficient inference for both models requires accurate estimation of nonparametric regression functions such as in the $n$-indexed sequence of models
\be\label{eq:mod1}
	y_i \ = \ f(\u_i) + \esi\,,
\ee
where for each $i\in[n]$, $y_i \in \R$ is a response, $\u_i = (u_1,\ldots,u_{\pu}) \in \Us \subseteq \R^{\pu}$ for $\pu \equiv \pun$ are exposures, $\esi \sim \Normal(0,\sd^2)$ with $\sd \equiv \sd_n$, and $f \equiv f_n : \Us \to \R$ is the target regression function. For brevity, we write $f_i := f(\u_i)$. 

In particular, efficient estimation of the treatment effect in the IV model depends crucially on consistent estimation of the conditional means of the endogenous variables given the instrumental variables; see \cite{A74}, \cite{N90}. In turn, it is important to identify conditions under which such estimation is possible. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% 
\subsection{Series estimation}\label{ss:se}

One strategy is to suppose that there exist transformations $\z_i \equiv \z_i(\u_i) := (z^j(\u_i))_{j\in[\pz]} \in \Zs \subseteq \R^{\pz}$ of $\u_i$ such that the regression function $f$ of model \eqref{eq:mod1} may be well approximated by some $\fb \equiv \fbn : \Zs \to \R$ belonging to a family $\famb := \{ f_b:b\in\Bs\}$ indexed by $\Bs \equiv \Bs_n \subseteq \R^{p_\Bs}$. If $\famb$ is suitably restricted, then a natural estimator of $\fb$ is $\fbhat := f_{\regbh}$, where $\regbh \equiv \regbh_n(\y, \Z)$ is an estimator of $\regb$ fit to the model $\E[y_i\,|\,\z_i] = \fb(\z_i)$ given the data $\y = (y_i)_{i\inn}, \Z = (\z^j)_{j\in[\pz]}$. For linear $\fb$, as we consider in the sequel, the respective $\fbhat$ are known as \emph{series estimators}. The nomenclature is due to the typical expansion of $\u$ into a series of basis functions $\z^j$, $j\in[\pz]$. 

Note that the aforementioned model is correct only up to the approximation errors $\remi \equiv \remi(\regb) := f(\u_i) - \fb(\z_i)$ for $i\inn$. Let $\rb$ denote the rate (up to order) of the scaled $\ell_2$ approximation error of $\fb$, so that the $\remi$ satisfy $\Exn^{1/2}[r_i] \lesssim_\Prb \rb$. Given an approximation rate $\rb$ and a sequence of such estimators $\regbh$, it is natural to ask: (i) how well do the $\regbh$ estimate $\regb$ despite such approximation error, and (ii) how well do the resultant $\fbhat$ estimate $f$? 

In well-known results, \cite{N97} answers such questions for linear $\fb$ under certain conditions on the smoothness of $f$ and the series terms $\z(\u)$. In particular, \cite{N97} studies $f$ for which there exist sequences of approximating $\regb$ such that the maximum partial derivatives of the approximation errors $\remi$ are of the order $\pz^{-\ga}$ for some $\ga > 0$. For such $f$, and for series terms $\z^j$ with well-behaved covariance matrices and which satisfy $\lim_{n\to\infty} \zeta(\pz)^2\pz/n = 0$, where $\zeta(\pz)$ is a bound on the magnitude of the series terms, \cite{N97} derives the following bound on the mean-squared error of the corresponding series estimator $\fhc \equiv \fhc_{\regb}$
\benn
	\int_{\Us} \{\fhc(\z(\u)) - f(\u)\}^2 dF(\u) \ = \ O_\Prb(\pz/n + \pz^{-2\ga}) \,.
\eenn
For $\z$ consisting of power series and splines, the above estimate leads to optimal convergence rates for the usual $\ell_p$ norm of the error $\fhc(\z(\u)) - f(\u)$, in the sense of \cite{S82}'s bounds on global rates for nonparametric regression. \cite{N97} also derives uniform convergence rates, which are not optimal for such $\z$. In turn, such rates establish $\sqrt{n}$-consistency for parameters given by pathwise-differentiable functionals of the conditional mean $f$, under further conditions suitable to the context.  

For our purposes, it is relevant to note that the requirement that $\lim_{n\to\infty} \zeta(\pz)^2\pz/n = 0$ necessitates that $\pz^2/n \to 0$ for in the case of $\z$ given by regression splines and $\pz^3/n \to 0$ for $\z$ given by power series bases. Thus, $f$ must be sufficiently smooth to be well-approximated by a suitably controlled number of series terms $\pz$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% IV SELECTION
\subsection{Selection of series terms}
\renewcommand{\P}{\bs{P}}

The effectiveness of series estimation requires good selection of series terms. Unfortunately, the aforementioned results say nothing about methods for empirical selection of the latter. One reasonable approach, as suggested by \cite{A74} and \cite{K71}, is simply to include ``lower-order'' polynomial transformations of the instrumental variables $\u^j$, $j\in[\pu]$, so that the number of terms increases modestly with $n$. Prima facie, there is ``room'' between the growth rate limits on the number of series terms $\pz$ imposed by, say, \cite{N97}, and the sequences $\pz$ that satisfy $\pz/n \to \ga \in [0, \infty)$ that can be handled by the estimators considered in, say, \cite{B94} and \cite{HHN08}. However, as \cite{H02} notes, the limiting distributions of the latter estimators may depend on $\ga$ and thereby may not reach the efficiency bound entailed by the former's convolution theorem. 

It would be unfortunate and ironic if, in order accommodate the many instrumental variables required to consistently estimate the conditional expectations of the endogenous variables, one needed to use estimators that accrue no efficiency gains for the effort. Indeed, \cite{H02} cites \cite{N97} as evidence that a parsimonious set of series terms $\z^j$ is preferable to a set ``many instrumental variables''. The former suggests a high-level condition that requires the existence of a projection matrix $\P$ close to that associated with $\Z$ but which satisfies $\rank \P = o(\sqrt{n})$. Per \cite{H02}, if a number (of the desired order) of columns $\z^j$ ``well-approximated'' (the image of) $\Z$, then one could use the projection associated with the matrix formed by such columns to form an efficient estimator of the treatment effect. This observation, though heuristic in nature, further highlights variable selection as a missing ingredient for effient estimation in the context of IV models.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% APPROXIMATE SPARSITY
\subsection{Approximate sparsity}\label{ss:sparsity}


The previous section presented a tension between the objectives of, one the one hand, including sufficiently many instrumental variables (or transformations thereof) in one's first-stage regression in order to accurately estimate the conditional means of the endogenous variables and, on the other hand, including sufficiently few instrumental variables so as not to negate any putative efficiency gains. The primary contribution of the presently studied paper, \cite{BCH11}, is to use $\ell_1$-regularized estimation in the first-stage to reconcile these competing objectives. 

We saw in Section \ref{ss:se} that rates of convergence for series estimators can be derived by imposing differentiability restrictions on the approximation errors $\remi$. \cite{BCH11} give similar results for linear $\fb$ that depend sparsely on their arguments. If, for a sequence $\sb \equiv \sb_n$ and the approximation rate $\rb = \sd\sqrt{\sb/n}$, there exists a sequence of $\fb$ each belonging to the respective family
\benn%\label{def:fams}
	\fams \ := \ \{ f_{\bs{b}}: \z \mapsto \langle \z, \bs{b}\rangle \,:\, \bs{b} \in \R^{\pz},\, \|\bs{b}\|_0 \leq \sb \} \,,
\eenn
then, under forthcoming conditions and a suitable choice of $\tune$, the Lasso estimator
\be\label{def:lasso}
	\regbh \ \in \ \argmin_{\bs{b}\,\in\,\R^{\pz}} \big\{ \|\y - \Z\bs{b}\|_2^2/n + \tune\|\bs{b}\|_1 \big\}
\ee
satisfies $\|\regbh - \regb\|_{\gramhat} \lesssim_\Prb \sd\sqrt{s\log(\pz\vee n)/n}$, where $\gramhat = \Exn[\z_i^{\kpt}]$ and hence obtains a prediction rate of order $\rb\sqrt{\log(\pz\vee n)}$. The estimator $\fhat := (\fbhat(\z_i))_{\inn}$ of $\f := (f_i)_{\inn}$ in turn satisfies
\begin{align}
	\| \fhat -\f \|_2 \ & \leq \  \|\fhat - \ffb\|_2 + \| \f - \fb \|_2 \nonumber\\
	& = \ \sqrt{n}\|\regbh - \regb\|_{\gramhat} + \|\rem\|_2 \ \lesssim_\Prb \ \sd\sqrt{\sb\log(\pz\vee n)} \,,
\end{align}
where $\ffb := (\fb(\z_i))_{i\inn}$. Under the typical assumptions that $\sd = O(1)$ and $\sb = o(n/\log\pz)$, the right-hand side above is~$o(\sqrt{n})$, which entails that $\fhat$ achieves ``consistency'' in (scaled) $\ell_2$-norm. The foregoing considerations suggest the following characterization of such models.

\begin{defi}\label{def:as}[Approximate sparsity] Given a sequence of models of the form \eqref{eq:mod1} and respective observations $(y_i, \u_i)_{i\inn} \in \R^{1\times\pu}$, we say that the regression functions $f$ are \emph{approximately sparse} relative to a sequence of transformations $\z \equiv \z_n(\u)$, where the $\z^j$ are considered fixed and scaled so that $\|\z^j\|_2 = \sqrt{n}$, and a sequence of approximating functions $\fb \equiv \fbn \in \fams$ for $\sb \equiv \sb_n$ if (i) $\sb = o(n/\log\pz)$ and (ii) the approximation errors $\remi(\regb)$ for $i\inn$ satisfy $\Exn^{1/2}[\remi^2] \lesssim \sd\sqrt{\sb/n}$.
\end{defi}

\begin{rek} That a sequence of regression functions is approximately sparse is equivalent to that Condition ASM of \cite{BCH11} holds. Our choice of presentation is motivated by concerns for clarity of the order of logical quantification over the different model parts. 
\end{rek}

\begin{rek} As \cite{BCH11} consider fixed instrumental variables $\z^j$, the remainders $\remi$ are not stochastic quantities; hence the deterministic rate condition of Definition \ref{def:as}. This contrasts with the setting of \cite{N97}. \cite{BCCH12} consider random $\z^j$ and hence present approximate sparsity in terms of a probabilistic bound on the empirical approximation error. 
\end{rek}

As we will codify in Section \ref{s:theory}, the upshot of Definition \ref{def:as} is that one may apply modern statistical learning techniques to estimate an approximately sparse regression function at a near-oracle rate without any prior knowledge of which series terms ought to be included in the model. In the IV context, this benefit generalizes to the situation in which many instrumental variables are immediately available, say as levels of a large factor. By using $\ell_1$-regularized estimation methods, we needn't necessarily be caught between large finite-sample bias due to the use of many instrumental variables and asymptotic inefficiency due to corrections for the former.  




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%% THEORETICAL RESULTS
\section{Theoretical results}\label{s:theory}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%% LASSO ESTIMATION
\subsection{Feasible Lasso estimators}\label{ss:feasible}
\newcommand{\minse}{\phi_{\mathrm{min}}}
\newcommand{\maxse}{\phi_{\mathrm{max}}}
\newcommand{\cminse}{\kappa_*}
\newcommand{\cmaxse}{\kappa^*}
\newcommand{\M}{\bs{M}}
\newcommand{\gd}{\delta}
\newcommand{\diff}{\bs{\gd}}

In this section, we briefly review the conditions that lead to good performance for Lasso estimators. There two primary ingredients: (i) suitable choice of the tuning parameter $\tune$, and (ii) conditions on the empirical Gram matrix $\gramhat = \Exn[\z_i^{\kpt}]$. Well-known oracle rates for the Lasso estimator fit to a model such as that of \eqref{eq:mod1} require that the tuning parameter dominate the ``noise'' term $\|\Exn[\z_i\esi]\|_\infty$. For Gaussian $\esi$, \cite{BC13} achieve as much with high probability by setting
\be\label{tune1}
%	\tune \ \geq \ 2 c \sd \Phi^{-1}(1-\gamma/2\pz)\,,
	\tune \ = \ 2 c \sd \Lambda(1-\gamma\,|\,\Z)\,,
\ee
where $c, \gamma$ are controlled tolerances and 
\begin{align*}
	\Lambda(1 - \gamma\,|\,\Z) \ & := \ (1-\gamma)\text{-quantile of } n\|\Exn[\z_ig_i]\|_\infty\\
	& \leq \ \sqrt{n}\Phi^{-1}(1-\gamma/2\pz) \ \leq \ \sqrt{2n\log(2\pz/\gamma)} \,,
\end{align*}
where $g_i \sim_{iid} \Normal(0,1), i\in[n]$ and the estimate of the right-hand side above features in the tuning parameter suggestion by \cite{BRT09}. In practice, one typically sets $c,\gamma$ to 1.1, 0.5, respectively; $\Lambda$ can be estimated easily via simulation. Furthermore, one must estimate the structural noise level $\sd$; \cite{BCH11} provide an iterative algorithm for this in their Appendix A. In the sequel, we refer, as do \cite{BCH11}, to Lasso estimators with tuning parameters informed by true and estimated structural noise levels as infeasible and feasible Lasso estimators, respectively. \cite{BCCH12} introduce a tuning parameter regime for the feasible Lasso in the case of non-Gaussian, heterscedastic structural noise. We do not consider their procedure here. 

[ DISCUSS FEASIBLE LASSO ESTIMATORS HERE ]

The aforementioned rates also required good behavior of certain moduli of continuity of the empirical Gram matrix. This condition appears in different forms in the literature --- for instance, the compatibility condition or restricted eigenvalues condition. \cite{BCH11} introduce it in terms of the minimal and maximal $m$-sparse eigenvalues of a semidefinite matrix $\M \in \R^{q \times q}$, defined respectively as
\benn
	\minse(m; M) \ := \ \min_{\substack{\diff\,\in\,\R^q\setminus\{\bs{0}\} \,: \\ \|\diff\|_0 \,\leq\, m}} \frac{\|\diff\|_{\M}}{\|\diff\|_2^2}\,, \qquad\qquad
	\maxse(m; M) \ := \ \max_{\substack{\diff\,\in\,\R^q\setminus\{\bs{0}\} \,: \\ \|\diff\|_0 \,\leq\, m}} \frac{\|\diff\|_{\M}}{\|\diff\|_2^2}\,.
\eenn

\begin{defi}[Sparse eigenvalues condition]
We say that the sequence of design matrices $\Z \equiv \Z_n$ with empirical Gram matrices $\gramhat$ satisfies the \emph{sparse eigenvalues (SE) condition} if there exists an unbounded sequence $h_n$ and finite universal constants $\cminse, \cmaxse > 0$ that satisfy
\benn
	\cminse \ \leq \minse(h_n s; \gramhat) \ \leq \ \maxse(h_n s; \gramhat) \ \leq \ \cmaxse \,.
\eenn
\end{defi}

The following adaptation of \cite[Theorem 1]{BC13}, which is in turn an extension of the work of \cite{BRT09}, demonstrates that the order of estimation and prediction error of the (feasible) Lasso does not suffer under the assumption of approximately sparse regression functions $f$.

\begin{thm}[{\cite[Theorem 1]{BCH11}}]\label{thm:1}
Let $(y_i, \u_i)_{i\inn} \in \R^{1\times\pu}$ be distributed according an $n$-indexed sequence of models of \eqref{eq:mod1}, and suppose that (i) the respective sequence regression functions $f$ are approximately sparse with respect to transformations $\z \equiv \z_{,n}(\u)$ and approximation parameters $\regb$; (ii) condition SE holds. Then, for $n$ sufficiently large, the (feasible) Lasso estimator $\regbh$ fit to the model $\E[y_i\,|\,\z_i] = \ip{\z_i,\regb}$ satisfies the following with probability at least $1-\gamma$:
\benn
	C_1\|\regbh - \regb\|_2 \ \leq \ \|\regbh-\regb\|_{\gramhat} \ \leq \ C_2 \sd \sqrt{s\log(2\pz/\gamma)/n} \,,
\eenn
where $C_1, C_2 > 0$ satisfy $C_1 \gtrsim \sqrt{\cminse}$ and $C_2 \lesssim 1/\sqrt{\cminse}$, and $\log(\pz/\gamma) \lesssim \log(\pz\vee n)$. 
\end{thm}

Theorem \ref{thm:1} provides the basis for the subsequent results concerning Lasso-based inference for nonlinear IV models with approximately sparse endogenous regression functions. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%% LASSO ESTIMATION
\subsection{Inference for IV models}
\newcommand{\regt}{\gth}
\newcommand{\regg}{\bs{\gamma}}
\newcommand{\rd}{\bs{\delta}}
\newcommand{\rdhat}{\hat{\rd}}
\newcommand{\efi}{v_i}
\newcommand{\efc}{v}
\newcommand{\esc}{u}
\newcommand{\sds}{\sd_\esc}
\newcommand{\sdf}{\sd_\efc}
\renewcommand{\a}{\bs{a}}
\newcommand{\ah}{\hat{\a}}
\newcommand{\sdfs}{\sd_{\esc\efc}}
\newcommand{\xinst}{\x^\dagger}
\newcommand{\xinsti}{x_i^\dagger}
\newcommand{\ainst}{\a^\dagger}
\newcommand{\ainsti}{\a_i^\dagger}
\newcommand{\ainstihat}{\hat{\a}_i^\dagger}
\newcommand{\ainstihatkpt}{\hat{\a}_i^{\dagger\kpt}}
\newcommand{\ainstikpt}{\a_i^{\dagger\kpt}}
\newcommand{\ainstkpt}{\a^{\dagger\kpt}}
\renewcommand{\rm}{\bs{\Delta}}
\newcommand{\g}{\bs{g}}
\renewcommand{\Q}{\bs{Q}}
\newcommand{\Qn}{\Q_n}
\newcommand{\Qnhat}{\hat{\Q}_n}
\newcommand{\Om}{\bs{\Omega}}
\newcommand{\Omn}{\Om_n}
\newcommand{\EEn}{\bar{\E}}
\newcommand{\rdiv}{\rdhat^\dagger}
\newcommand{\sdshat}{\hat{\sds}}


Consider an $n$-indexed sequence of IV models given by
\be\label{mod:iv}
	y_i \ = \ x_i\regt + \langle\w_i, \regg\rangle + \esi\,, \qquad x_i \ = \ d(\u_i) + \efi \,,
\ee
where $y_i \in \R$ is a response, $x_i \in \R$ is an endogenous treatment variable, $\w_i\in\R^{\pw}$ are exogenous ``control'' variables, $\u_i \in \R^{\pz}$ are instrumental variables that may intersect non-trivially with $\w_i$, $d$ is an unknown regression function, and the errors $\efi, \esi$ satisfy
\benn
	\begin{pmatrix} \efi \,|\, \u_i \\[-.5em] \esi \,|\, \u_i \end{pmatrix} \ \sim \ \Normal\left(\bs{0}, \begin{pmatrix} \sdf^2 & \sdfs \\[-.5em] \sdfs & \sds^2 \end{pmatrix} \right)\,.
\eenn
Our objective is efficient inference for $\regt$. For brevity, write $\a_i := (x_i, \w_i), \rd := (\regt, \regg)$ so that $y_i = \langle \a_i, \rd \rangle + \esi$. If $\xinst \equiv (\xinsti)_{i\inn}$ are instruments for $\x \equiv (x_i)_{i\inn}$, then the standard IV estimator is given by
\be\label{def:raiv}
	\rdiv(\xinst) \ = \ (\Exn[\ainsti(\xinsti) \kp \a_i])^{-1}\Exn[\ainsti(\xinsti) y_i] \,,
\ee
where $\ainst_i(\xinsti) := (\xinsti, \w_i)$. Under ``standard conditions,'' $\rdiv(\xinst)$ satisfies
\benn
	\sqrt{n}(\rdiv(\xinst) - \rd) \ = \ \g + \rm\,, \qquad \g \ \sim \ \Normal(\bs{0}, \Qn^{-1}\Omn\Qn^{-1\tp})\,,
\eenn
where $\Qn \equiv \Qn(\xinst) := \En[\ainst_i(\xinsti) \kp \a_i]$, $\Omn \equiv \Omn(\xinst) := \sds^2\En[\ainsti(\xinsti)^{\kpt}]$, and $\|\rm\|_\infty = o_{\Prb}(1)$. If $\xinsti = d_i := d(\u_i)$, then $\E[\xinsti x_i] = \E[d_i^2 + d_i\efi] = \E[d_i^2]$, in which case $\Qn = \Omn/\sds^2$ and hence
\benn
	\var(\g) \ = \ \sds^2\Qn^{-1} \ = \ \sds^2\big(\En\big[\ainsti(d_i)^{\kpt}\big]\big)^{-1} \,,
\eenn
the semiparametric efficiency bound for estimating $\rd$ --- c.f. \cite{BCCH12}, \cite{A74}, \cite{C87}, \cite{N90}. The $d_i$ are therefore regarded as \emph{optimal instruments}. In the sequel, we let unqualified use of $\ainsti$ refer to $\ainsti(d_i)$. 

In practice, the $d_i$ are not available and estimates $\dhati$ must be used instead. The foregoing discussion suggests that, in order for $\rdiv(\dhat)$ to achieve asymptotic efficiency, $\dhat$ must ``consistently'' estimate $\d$ at a sufficiently fast rate. The following theorem of \cite{BCH11} demonstrates that, if the regression functions $d$ are approximately sparse, then Lasso-based estimates suffice.

% Theorem 3
\begin{thm}[{\cite[Theorem 3]{BCH11}}]\label{thm:3}
Let $(y_i, \a_i, \u_i)_{i\inn}$ be distributed according to \eqref{mod:iv} for each $n$. Suppose that: (i) the regression functions $d$ are approximately sparse with respect to transformations $\z(\u) \in \R^{\pz}$ and approximation parameters $\regb$; (ii) $\dhat = (\langle \z_i,\regbh\rangle)_{i\inn}$, where $\regbh$ is a feasible (Post-) Lasso estimator of $\regb$; (iii) $\sdf, \sds$ and the eigenvalues of $\Qn \equiv \Qn(\dhat)$ are bounded strictly away from zero and infinity uniformly in $n$; (iv) Condition SE holds for $\gramhat$ with high probability; (v) $s = \supp(\regb)$ and $\pz$ satisfy $\sb^2\log^2(\pz\vee n) = o(n)$. Then, the IV estimator $\rdiv \equiv \rdiv(\dhat)$ satisfies
\benn
	(\sds^2\Qn^{-1})^{-1/2} \sqrt{n}(\rdiv-\rd) \ = \ \g + \rm\,, \qquad \g \ \sim \ \Normal(\bs{0}, \Id) \,,
\eenn
where $\|\rm\|_\infty = o_\Prb(1)$. Moreover, the result continues to hold if $\Qn$ is replaced by $\Qnhat := \Exn\big[\ainstihatkpt\big]$ and $\sds^2$ by $\sdshat^2 := \Exn\big[(y_i -\langle \ainstihat, \rdiv\rangle)^2\big]$, where $\ainstihat := \ainsti(\dhat)$. 
\end{thm}

The upshot of the foregoing theorem is that nonlinear IV estimation needn't incur efficiency penalties for high-dimensional series estimation of the endogenous regression functions. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%% SUP-SCORE STATISTIC
\subsection{Weak instruments and the sup-score statistic}\label{ss:supscore}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% SECTION: RESULTS
\section{Empirical results}\label{s:results}

We present the results of simulation studies of the Lasso-based IV estimators in Section \ref{ss:simulations}. We then apply the foregoing methods to the problem of IV selection in \cite{AK91}, who use IV methods to estimate the returns to schooling in terms of log wage in adult life. Christian Hansen was kind enough to provide us with the MATLAB source code used to conduct the original simulations and the econometric analysis. 

\subsection{Simulations}\label{ss:simulations}
\newcommand{\rpc}{\pi}
\newcommand{\rp}{\bs{\rpc}}
\newcommand{\rpj}{\rpc_j}
\newcommand{\rS}{\bs{\Sigma}}
\newcommand{\0}{\bs{0}}
\newcommand{\rt}{\theta}
\newcommand{\rtiv}{\hat{\rt}^\dagger}
\newcommand{\sdz}{\sigma_{z}}
\newcommand{\vrz}{\sdz^2}
\newcommand{\vry}{\sds^2}
\newcommand{\vrv}{\sdf^2}
\newcommand{\corr}{\mathrm{corr}}
\newcommand{\seh}{\hat{\mathrm{SE}}}
\newcommand{\ybar}{\bar{y}}
\newcommand{\fga}{\hat{\alpha}}
\newcommand{\lga}{\tilde{\alpha}}

\newcommand{\dhi}{\hat{d}_i}
\newcommand{\rdfl}{\hat{\rd}_{\fga}}
\newcommand{\rppl}{\hat{\rp}_{\mathrm{PL}}}
\newcommand{\rph}{\hat{\rp}}
\newcommand{\rphc}{\hat{\rpc}}
\newcommand{\Sh}{\hat{S}}
\newcommand{\rtfl}{\hat{\rt}_{\fga}}
\newcommand{\PS}{\P_{\Sh}}

%\newcommand{\raiv}

We present the results of Monte Carlo experiments to evaluate the finite-sample performance of the methods suggested in Section \ref{s:methods} relative to Theorems \ref{thm:3} and .... The experiment design and parameter configurations are as specified in \cite[Section 5.3]{BCH11}. We focus in particular on discrepancies between ...  

For each parameter configuration, we generate 500 generate samples of size $n \in \{100, 500\}$ according to the data-generating mechanism
\benn
\begin{split}
	\z_i \ & \sim \ \Normal(\0, \rS) \\
	x_i \ & = \ \ipp{\z_i,\rp}{} +\efi \\
	y_i \ & = \ \rt x_i + \esi \,,
\end{split}\qquad\qquad
\begin{split}
	\begin{pmatrix} \esi \\[-.5em] \efi \end{pmatrix}\,\big{|}\,\z_i \ \sim \ \Normal\left(\bs{0}, \begin{pmatrix} \sds^2 & \sdfs \\[-.5em] \sdfs & 		\sdf^2 \end{pmatrix} \right)\,,
\end{split}
\eenn
where $\rt = 1$ is the parameter of interest, $y_i\in\R$ are the responses, $x_i\in\R$ is an endogenous variable and $\z_i\in\R^{100}$ are instrumental variables. We fix the following over all configurations: $\rS$ is given by $\rS_{jj} = \vrz = 1$ and $\corr(z_{ij}, z_{ik}) = .5^{|j-k|}$; we set $\vry = 1$ and $\corr(\esi, \efi \,|\, \z_i) = .3$. The remaining parameters may vary amongst trials.

We consider four levels of the first-stage noise $\vrv$, based on ``signal levels'' $F^* \in \{0, 10, 40, 160\}$. At $F^* = 0$ , the level of ``no signal'', we set $\rp = \0$ and $\vrv = 1$. For the remaining levels, we set $\rpj = .7^{j-1}$ for each $j\in[100]$ and choose $\vrv$ to satisfy $\vrv = \frac{n\ipp{\rp, \rS\rp}{}}{F^*\ipp{\rp, \rp}{}}$\,. Our choices of $\rpj$ are motivated by the discussion of approximate sparsity in Section \ref{ss:sparsity}. 


We compute the method of moments IV estimator $\rtiv$
%\benn
%	\rtiv \ = \ \big(\En[\dhi x_i]\big)^{-1} \En[\dhi y_i] \,, \qquad \seh(\rtiv) \ = \  \big(\En[(y_i - x_i\rtiv)^2] \En[\dhi^2]^{-1}\big)^{1/2}
%\eenn
where the $y_i$ and $x_i$ are first centered and where
\vspace{-.7em}
\begin{align*}
% 	\dhi \ = \ \ip{\z_i, \rppl} \ = \ \z_{\Sh,i}'(\Z_{\Sh}'\Z_{\Sh})^{-1}\Z_{\Sh}'\x\,, \qquad
 	\dh \ = \ \ip{\z_i, \rppl} \ & = \ \P_{\Sh}\x\,, 
	 & \P_{\Sh} = \Z_{\Sh} (\Z_{\Sh}'\Z_{\Sh})^{-1}\Z_{\Sh}' 
\end{align*}
is the estimate of $\E[\x\,|\,\Z]$ due to the Post-Lasso estimate $\rppl$. Recall that the latter is the ordinary least squares fit of $\x$ to the restricted design matrix $\Z_{\Sh} = (\z^j)_{j \in \Sh}$, where $\Sh = \{j\in[\pz] : \rphc_j \neq 0\}$ is the active set of the first-stage Lasso estimator $\rph$. Because of the linearity in the second-stage and the use of the Post-Lasso to estimate $\d$, the IV estimator $\rtiv$ is precisely the 2SLS fit of $\y$ to $\x, \Z_{\Sh}$. 

Given an active set $\Sh$ due to a first-stage Lasso fit, we may also compute Fuller's modified LIML estimator based on $\Z_{\Sh}$: 
\be\label{eq:fuller}
%	\rtfl \ = \ \big(\En[x_i\dhi - \fga x_i^2]\big)^{-1}\En[x_i \yt - \fga x_i y_i
	\rtfl \ = \ (\x' \PS\x - \fga\x'\x)^{-1}(\x \PS\y - \fga\x'\y) \,,
\ee
where
\be\label{eq:fga}
	\fga \ = \ \frac{\lga - (1 - \lga) C / (n - \pz - \px)}{1 - (1 - \lga) C / (n - \pz - \px)} \,,
\ee
\vspace{.2em}the quantity $\lga$ is the smallest eigenvalue of the matrix $\big([\y,\x]'[\y,\x]\big)^{-1}[\y,\x]'\PS[\y,\x]$, and $C$ is a controlled constant that bears on a bias-variance tradeoff (see \cite[Theorem 1, Corollary 2]{F77}). Following \cite{BCH11}, we set $C=1$, which entails that the higher-order bias of $\rtfl$ is 0. The LIML estimator itself is given by \eqref{eq:fuller} for $\fga = \lga$. 

As remarked above, $\rtfl$ is a moment correction for the LIML estimator. The cost of this correction is the accuracy of the standard error estimates based on first order asymptotic approximations of $\var(\rtfl)$. Following \cite{BCH11}, we calculate the ``corrected'' standard errors of \cite{HHN08} based on those of \cite{B94}. The relevant formulas are in Appendix \ref{a:becker}. 

For each trial, we fit each of the 2SLS and Fuller estimators based on both the iterated and cross-validated Lasso estimators described in Section \ref{ss:feasible}. We denote these estimators as 2SLS(Lasso-IL), 2SLS(Lasso-CV), Fuller(Lasso-IL), and Fuller(Lasso-CV), in the obvious order. We also fit 2SLS and Fuller estimators using as many instrumental variables $\z^j$ as the present configuration allows. When $n=100$, we randomly choose 98 variables $\z^j$ for fitting; when $n=500$, we include all the $\z^j$. We denote these estimators by 2SLS(All) and Fuller(All), respectively. For each estimator and parameter configuration, we compute the root-mean-square-error (RMSE), the median entrywise bias (Med. Bias), the empirical rejection probability of level 0.05 Wald tests (rp(0.05)), and the number of trials for which the first-stage Lasso estimator selected zero instrumental variables ($\|\rph\|_0 = 0$). Finally, for each trial we conducted a level 0.05 test of the hypothesis that $\rt = 1$ using the sup-score statistic of Section \ref{ss:supscore}. The rejection probabilities of the latter are tabulated under rp(0.05). We present the original results of \cite{BCH11} (in red) alongside our own (in black). In the interest of space, we defer the results for the $n=500$ trials to Appendix \ref{a:simulations}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TABLES
\renewcommand{\baselinestretch}{1} 

\definecolor{gray1}{gray}{0.85}
\definecolor{darkred}{rgb}{.75,0,0}
\newcolumntype{a}{>{\color{darkred}}c}
\newcolumntype{b}{>{\color{darkred}}r}
\newcommand{\sr}{\rule[-0.45cm]{0pt}{0.9cm}}

\begin{table}[t!]
\centering
\caption{IV simulation results ($n = 100, F^* \in\{0, 10\})$}

\small{
\makebox[\textwidth][c]{
\renewcommand{\arraystretch}{1.25}%
\def\sym#1{\ifmmode^{#1}\else\(^{#1}\)\fi}%
\rowcolors{2}{gray!10}{white}
\begin{tabular}{lcacacacarb}
\toprule
& \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{Med. Bias} & \multicolumn{2}{c}{rp(0.05)} & \multicolumn{2}{c}{$\|\rph\|_0 = 0$} \\ 
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
\rowcolor{white!0} & \multicolumn{1}{c}{D.G.} & \multicolumn{1}{a}{A.B. et al.} &
\multicolumn{1}{c}{D.G.} & \multicolumn{1}{a}{A.B. et al.} &
\multicolumn{1}{c}{D.G.} & \multicolumn{1}{a}{A.B. et al.} &
\multicolumn{1}{c}{D.G.} & \multicolumn{1}{a}{A.B. et al.} \\
\midrule
\rowcolor{white} No Signal \\
  \cmidrule[.06em](lr{2em}){1-1}
2SLS(All) & 0.311 & 0.318 & 0.296 & 0.305 & 0.876 & 0.862 &  &  \\ 
  Fuller(All) & 1.957 & 2.398 & 0.369 & 0.248 & 0.714 & 0.704 &  &  \\ 
  IV(Lasso-IL) & 0.442 & 0.511 & 0.315 & 0.338 & 0.124 & 0.014 & 486 & 455 \\ 
  Fuller(Lasso-IL) & 0.422 & 0.509 & 0.313 & 0.338 & 0.096 & 0.010 & 486 & 455 \\ 
  IV(Lasso-CV) & 0.333 & 0.329 & 0.296 & 0.301 & 0.466 & 0.652 & 0 & 0 \\ 
  Fuller(Lasso-CV) & 0.353 & 0.359 & 0.295 & 0.305 & 0.288 & 0.384 & 0 & 0 \\ 
  Sup-Score &  &  &  &  & 0.052 & 0.004 &  &  \\ 
  \addlinespace[.5em]
  \rowcolor{white} $F^* = 10$ \\
  \cmidrule[.06em](lr{2em}){1-1}
  2SLS(All) & 0.060 & 0.058 & 0.056 & 0.058 & 0.806 & 0.806 &  &  \\ 
  Fuller(All) & 0.525 & 0.545 & 0.041 & 0.050 & 0.734 & 0.690 &  &  \\ 
  IV(Lasso-IL) & 0.062 & 0.055 & 0.022 & 0.020 & 0.086 & 0.042 & 233 & 147 \\ 
  Fuller(Lasso-IL) & 0.060 & 0.054 & 0.024 & 0.020 & 0.072 & 0.032 & 233 & 147 \\ 
  IV(Lasso-CV) & 0.052 & 0.052 & 0.039 & 0.024 & 0.260 & 0.072 & 0 & 10 \\ 
  Fuller(Lasso-CV) & 0.054 & 0.051 & 0.036 & 0.022 & 0.180 & 0.068 & 0 & 10 \\ 
  Sup-Score &  &  &  &  & 0.044 & 0.006 &  &  \\ 
\bottomrule
\end{tabular}
} % make box
} % small
\end{table}

\begin{table}[t!]
\centering
\caption{IV simulation results ($n = 100, F^* \in\{40, 160\})$}

\small{
\makebox[\textwidth][c]{
\renewcommand{\arraystretch}{1.25}%
\def\sym#1{\ifmmode^{#1}\else\(^{#1}\)\fi}%
\rowcolors{2}{gray!10}{white}
\begin{tabular}{lcacacacarb}
\toprule
& \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{Med. Bias} & \multicolumn{2}{c}{rp(0.05)} & \multicolumn{2}{c}{$\|\rph\|_0 = 0$} \\ 
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
\rowcolor{white!0} & \multicolumn{1}{c}{D.G.} & \multicolumn{1}{a}{A.B. et al.} &
\multicolumn{1}{c}{D.G.} & \multicolumn{1}{a}{A.B. et al.} &
\multicolumn{1}{c}{D.G.} & \multicolumn{1}{a}{A.B. et al.} &
\multicolumn{1}{c}{D.G.} & \multicolumn{1}{a}{A.B. et al.} \\
\midrule
  \rowcolor{white} $F^* = 40$ \\
  \cmidrule[.06em](lr{2em}){1-1}
  2SLS(All) & 0.081 & 0.081 & 0.076 & 0.072 & 0.654 & 0.626 &  &  \\ 
  Fuller(All) & 0.660 & 0.951 & 0.031 & 0.050 & 0.758 & 0.690 &  &  \\ 
  IV(Lasso-IL) & 0.055 & 0.051 & 0.011 & 0.012 & 0.064 & 0.048 &    1 & 1 \\ 
  Fuller(Lasso-IL) & 0.056 & 0.051 & 0.009 & 0.011 & 0.056 & 0.046 &    1 & 1 \\ 
  IV(Lasso-CV) & 0.054 & 0.048 & 0.012 & 0.016 & 0.070 & 0.058 &    0 & 0 \\ 
  Fuller(Lasso-CV) & 0.055 & 0.049 & 0.009 & 0.014 & 0.060 & 0.050 &    0 & 0 \\ 
  Sup-Score &  &  &  &  & 0.042 & 0.004 &  &  \\ 
  \addlinespace[.5em]
  \rowcolor{white} $F^* = 160$ \\
  \cmidrule[.06em](lr{2em}){1-1}
  2SLS(All) & 0.079 & 0.075 & 0.063 & 0.062 & 0.350 & 0.306 &  &  \\ 
  Fuller(All) & 1.062 & 1.106 & 0.015 & 0.023 & 0.698 & 0.622 &  &  \\ 
  IV(Lasso-IL) & 0.052 & 0.049 & 0.008 & 0.005 & 0.070 & 0.064 &    0 & 0 \\ 
  Fuller(Lasso-IL) & 0.053 & 0.049 & 0.006 & 0.002 & 0.066 & 0.056 &    0 & 0 \\ 
  IV(Lasso-CV) & 0.054 & 0.048 & 0.007 & 0.006 & 0.058 & 0.054 &    0 & 0 \\ 
  Fuller(Lasso-CV) & 0.054 & 0.049 & 0.006 & 0.003 & 0.058 & 0.048 &    0 & 0 \\ 
  Sup-Score &  &  &  &  & 0.046 & 0.004 &  &  \\ 
\bottomrule
\end{tabular}
} % make box
} % small
\end{table}

\renewcommand{\baselinestretch}{1.5} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% 	SECTION: EMPIRICAL RESULTS 
\subsection{Empirical results: returns to schooling}

We apply the methodology developed in Sections \ref{s:methods} and \ref{s:theory} to conduct inference for the effect of years of schooling on log wage. The data consist of  U.S. Census 





\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% APPENDIX A -- LASSO ITERATION METHOD


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% APPENDIX B -- HHN08 STANDARD ERRORS
\section{Modified LIML estimator standard errors}\label{a:becker}
\newcommand{\uh}{\hat{\u}}
\newcommand{\uhc}{\hat{u}}
\newcommand{\uhi}{\uhc_i}
\newcommand{\sdsh}{\hat{\sds}}

\cite{HHN08} extend the work of \cite{B94} to give corrected standard errors for the Fuller, or modified LIML, estimator given by \eqref{eq:fuller}, \eqref{eq:fga}. Let ...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% APPENDIX C -- Simulation Results
\section{IV simulation results}\label{a:simulations}

\renewcommand{\baselinestretch}{1} 
% 7
\begin{table}[h!]
\renewcommand{\arraystretch}{1.25}%
\centering
\caption{IV simulation results ($n = 500, F^* \in\{0, 10, 40, 160\})$}
\small{
\makebox[\textwidth][c]{
\def\sym#1{\ifmmode^{#1}\else\(^{#1}\)\fi}%
\rowcolors{2}{gray!10}{white}
\begin{tabular}{lcacacacarb}
\toprule
%\multirow{2}{*}{\raisebox{-.3em}{\scalebox{1.2}{$n=100$}}} 
& \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{Med. Bias} & \multicolumn{2}{c}{rp(0.05)} & \multicolumn{2}{c}{$\|\rph\|_0 = 0$} \\ 
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
\rowcolor{white!0} & \multicolumn{1}{c}{D.G.} & \multicolumn{1}{a}{A.B. et al.} &
\multicolumn{1}{c}{D.G.} & \multicolumn{1}{a}{A.B. et al.} &
\multicolumn{1}{c}{D.G.} & \multicolumn{1}{a}{A.B. et al.} &
\multicolumn{1}{c}{D.G.} & \multicolumn{1}{a}{A.B. et al.} \\
\midrule
\rowcolor{white} No Signal \\
  \cmidrule[.06em](lr{2em}){1-1}
2SLS(All) & 0.312 & 0.312 & 0.297 & 0.297 & 0.860 & 0.852 &  &  \\ 
  Fuller(All) & 1.255 & 1.236 & 0.282 & 0.318 & 0.074 & 0.066 &  &  \\ 
  IV(Lasso-IL) & 0.421 & 0.477 & 0.279 & 0.296 & 0.152 & 0.012 &  494 & 486 \\ 
  Fuller(Lasso-IL) & 0.406 & 0.477 & 0.279 & 0.296 & 0.104 & 0.012 &  494 & 486 \\ 
  IV(Lasso-CV) & 0.327 & 0.478 & 0.302 & 0.299 & 0.546 & 0.064 &    0 & 348 \\ 
  Fuller(Lasso-CV) & 0.355 & 0.474 & 0.303 & 0.299 & 0.346 & 0.054 &    0 & 348 \\ 
  Sup-Score &  &  &  &  & 0.040 & 0.010 &  &  \\ 
  \addlinespace[.5em]
  \rowcolor{white} $F^* = 10$ \\
  \cmidrule[.06em](lr{2em}){1-1}
  2SLS(All) & 0.026 & 0.026 & 0.024 & 0.025 & 0.782 & 0.808 &  &  \\ 
  Fuller(All) & 0.073 & 0.816 & 0.004 & 0.006 & 0.052 & 0.052 &  &  \\ 
  IV(Lasso-IL) & 0.024 & 0.027 & 0.008 & 0.009 & 0.070 & 0.056 &  218 & 160 \\ 
  Fuller(Lasso-IL) & 0.023 & 0.027 & 0.009 & 0.009 & 0.060 & 0.044 &  218 & 160 \\ 
  IV(Lasso-CV) & 0.021 & 0.027 & 0.016 & 0.009 & 0.216 & 0.054 &    0 & 202 \\ 
  Fuller(Lasso-CV) & 0.021 & 0.027 & 0.014 & 0.009 & 0.132 & 0.044 &    0 & 202 \\ 
  Sup-Score &  &  &  &  & 0.036 & 0.004 &  &  \\ 
  \addlinespace[.5em]
  \rowcolor{white} $F^* = 40$ \\
  \cmidrule[.06em](lr{2em}){1-1}
  2SLS(All) & 0.036 & 0.036 & 0.033 & 0.032 & 0.636 & 0.636 &  &  \\ 
  Fuller(All) & 0.042 & 0.038 & -0.000 & 0.000 & 0.038 & 0.036 &  &  \\ 
  IV(Lasso-IL) & 0.024 & 0.022 & 0.005 & 0.003 & 0.048 & 0.048 &    0 & 0 \\ 
  Fuller(Lasso-IL) & 0.024 & 0.022 & 0.004 & 0.002 & 0.042 & 0.038 &    0 & 0 \\ 
  IV(Lasso-CV) & 0.023 & 0.022 & 0.005 & 0.004 & 0.048 & 0.052 &    0 & 0 \\ 
  Fuller(Lasso-CV) & 0.023 & 0.022 & 0.004 & 0.003 & 0.038 & 0.042 &    0 & 0 \\ 
  Sup-Score &  &  &  &  & 0.056 & 0.006 &  &  \\ 
  \addlinespace[.5em]
  \rowcolor{white} $F^* = 160$ \\
  \cmidrule[.06em](lr{2em}){1-1}
  2SLS(All) & 0.034 & 0.034 & 0.029 & 0.029 & 0.368 & 0.334 &  &  \\ 
  Fuller(All) & 0.025 & 0.026 & 0.002 & 0.002 & 0.028 & 0.044 &  &  \\ 
  IV(Lasso-IL) & 0.022 & 0.022 & 0.004 & 0.002 & 0.050 & 0.044 &    0 & 0 \\ 
  Fuller(Lasso-IL) & 0.022 & 0.022 & 0.003 & 0.001 & 0.046 & 0.040 &    0 & 0 \\ 
  IV(Lasso-CV) & 0.022 & 0.022 & 0.003 & 0.002 & 0.044 & 0.040 &    0 & 0 \\ 
  Fuller(Lasso-CV) & 0.022 & 0.022 & 0.002 & 0.000 & 0.044 & 0.038 &    0 & 0 \\ 
  Sup-Score &  &  &  &  & 0.060 & 0.010 &  &  \\ 
\bottomrule
\end{tabular}
} % make box
} % small
\end{table}
\renewcommand{\baselinestretch}{1.5} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% REFERENCES

\bibliographystyle{plainnat}
\bibliography{../refs/refs}



\end{document}